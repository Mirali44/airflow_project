{"timestamp":"2025-09-03T13:56:02.266258","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-03T13:56:02.267410","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/test.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-03T13:56:02.301129","level":"info","event":"Secrets backends loaded for worker","count":1,"backend_classes":["EnvironmentVariablesBackend"],"logger":"supervisor"}
{"timestamp":"2025-09-03T13:56:02.303074","level":"info","event":"Connection Retrieved 'spark_default'","logger":"airflow.hooks.base"}
{"timestamp":"2025-09-03T13:56:02.303907","level":"info","event":"Spark-Submit cmd: spark-submit --master spark-master:7077 --total-executor-cores 1 --executor-memory 512m --name arrow-spark --verbose /opt/airflow/dags/spark_minio_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:04.819595","level":"info","event":"Using properties file: /opt/spark/conf/spark-defaults.conf","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.085969","level":"info","event":"Adding default property: spark.delta.logStore.class=org.apache.spark.sql.delta.storage.S3SingleDriverLogStore","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.086385","level":"info","event":"Adding default property: spark.history.fs.logDirectory=s3a://spark/","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.086614","level":"info","event":"Adding default property: spark.eventLog.enabled=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.086826","level":"info","event":"Adding default property: spark.sql.files.ignoreMissingFiles=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.086977","level":"info","event":"Adding default property: spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.087215","level":"info","event":"Adding default property: spark.hadoop.fs.s3a.path.style.access=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.087361","level":"info","event":"Adding default property: spark.network.timeout=10000s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.087484","level":"info","event":"Adding default property: spark.hadoop.fs.s3a.secret.key=*********(redacted)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.087663","level":"info","event":"Adding default property: spark.hadoop.fs.s3a.multiobjectdelete.enable=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.087826","level":"info","event":"Adding default property: spark.logConf=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.087946","level":"info","event":"Adding default property: spark.hadoop.fs.s3a.access.key=*********(redacted)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.088104","level":"info","event":"Adding default property: spark.sql.sources.partitionOverwriteMode=dynamic","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.088246","level":"info","event":"Adding default property: spark.eventLog.dir=/opt/spark/history","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.088565","level":"info","event":"Adding default property: spark.hadoop.fs.s3a.endpoint=http://minio:9000","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.088709","level":"info","event":"Adding default property: spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.088827","level":"info","event":"Adding default property: spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.088994","level":"info","event":"Adding default property: spark.hadoop.fs.s3a.fast.upload=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.134248","level":"info","event":"Parsed arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.134744","level":"info","event":"master                  spark-master:7077","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.134912","level":"info","event":"remote                  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.135075","level":"info","event":"deployMode              null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.135307","level":"info","event":"executorMemory          512m","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.135459","level":"info","event":"executorCores           null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.135650","level":"info","event":"totalExecutorCores      1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.135781","level":"info","event":"propertiesFile          /opt/spark/conf/spark-defaults.conf","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.135898","level":"info","event":"driverMemory            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.136259","level":"info","event":"driverCores             null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.136524","level":"info","event":"driverExtraClassPath    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.136676","level":"info","event":"driverExtraLibraryPath  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.136922","level":"info","event":"driverExtraJavaOptions  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.137151","level":"info","event":"supervise               false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.137400","level":"info","event":"queue                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.137804","level":"info","event":"numExecutors            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.138103","level":"info","event":"files                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.138464","level":"info","event":"pyFiles                 null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.138707","level":"info","event":"archives                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.139138","level":"info","event":"mainClass               null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.139388","level":"info","event":"primaryResource         file:/opt/airflow/dags/spark_minio_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.139678","level":"info","event":"name                    arrow-spark","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.139974","level":"info","event":"childArgs               []","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.140204","level":"info","event":"jars                    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.140483","level":"info","event":"packages                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.140699","level":"info","event":"packagesExclusions      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.141029","level":"info","event":"repositories            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.141275","level":"info","event":"verbose                 true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.141497","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.141803","level":"info","event":"Spark properties used, including those specified through","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.142063","level":"info","event":"--conf and those from the properties file /opt/spark/conf/spark-defaults.conf:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.142595","level":"info","event":"(spark.delta.logStore.class,org.apache.spark.sql.delta.storage.S3SingleDriverLogStore)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.142890","level":"info","event":"(spark.eventLog.dir,/opt/spark/history)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.143224","level":"info","event":"(spark.eventLog.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.143500","level":"info","event":"(spark.hadoop.fs.s3a.access.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.144019","level":"info","event":"(spark.hadoop.fs.s3a.endpoint,http://minio:9000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.144321","level":"info","event":"(spark.hadoop.fs.s3a.fast.upload,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.144574","level":"info","event":"(spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.144857","level":"info","event":"(spark.hadoop.fs.s3a.multiobjectdelete.enable,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.145022","level":"info","event":"(spark.hadoop.fs.s3a.path.style.access,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.145261","level":"info","event":"(spark.hadoop.fs.s3a.secret.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.145405","level":"info","event":"(spark.history.fs.logDirectory,s3a://spark/)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.145583","level":"info","event":"(spark.logConf,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.145719","level":"info","event":"(spark.network.timeout,10000s)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.145947","level":"info","event":"(spark.sql.catalog.spark_catalog,org.apache.spark.sql.delta.catalog.DeltaCatalog)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.146091","level":"info","event":"(spark.sql.extensions,io.delta.sql.DeltaSparkSessionExtension)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.146299","level":"info","event":"(spark.sql.files.ignoreMissingFiles,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.146459","level":"info","event":"(spark.sql.sources.partitionOverwriteMode,dynamic)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.146887","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:05.147042","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.006488","level":"info","event":"Main class:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.006837","level":"info","event":"org.apache.spark.deploy.PythonRunner","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.007604","level":"info","event":"Arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.008078","level":"info","event":"file:/opt/airflow/dags/spark_minio_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.008360","level":"info","event":"null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.015346","level":"info","event":"Spark config:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.016000","level":"info","event":"(spark.app.name,arrow-spark)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.016398","level":"info","event":"(spark.app.submitTime,1756907765961)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.016655","level":"info","event":"(spark.cores.max,1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.016930","level":"info","event":"(spark.delta.logStore.class,org.apache.spark.sql.delta.storage.S3SingleDriverLogStore)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.017167","level":"info","event":"(spark.eventLog.dir,/opt/spark/history)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.017377","level":"info","event":"(spark.eventLog.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.017623","level":"info","event":"(spark.executor.memory,512m)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.017907","level":"info","event":"(spark.hadoop.fs.s3a.access.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.018149","level":"info","event":"(spark.hadoop.fs.s3a.endpoint,http://minio:9000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.018465","level":"info","event":"(spark.hadoop.fs.s3a.fast.upload,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.018695","level":"info","event":"(spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.018976","level":"info","event":"(spark.hadoop.fs.s3a.multiobjectdelete.enable,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.019232","level":"info","event":"(spark.hadoop.fs.s3a.path.style.access,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.019487","level":"info","event":"(spark.hadoop.fs.s3a.secret.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.019836","level":"info","event":"(spark.history.fs.logDirectory,s3a://spark/)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.020128","level":"info","event":"(spark.logConf,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.020372","level":"info","event":"(spark.master,spark-master:7077)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.020613","level":"info","event":"(spark.network.timeout,10000s)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.020909","level":"info","event":"(spark.sql.catalog.spark_catalog,org.apache.spark.sql.delta.catalog.DeltaCatalog)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.021180","level":"info","event":"(spark.sql.extensions,io.delta.sql.DeltaSparkSessionExtension)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.021444","level":"info","event":"(spark.sql.files.ignoreMissingFiles,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.021676","level":"info","event":"(spark.sql.sources.partitionOverwriteMode,dynamic)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.021972","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.022265","level":"info","event":"(spark.submit.pyFiles,)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.022558","level":"info","event":"Classpath elements:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.023399","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.024356","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:06.024775","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.226722","level":"info","event":"25/09/03 13:56:07 INFO SparkContext: Running Spark version 3.5.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.229486","level":"info","event":"25/09/03 13:56:07 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.229948","level":"info","event":"25/09/03 13:56:07 INFO SparkContext: Java version 17.0.16","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.330674","level":"info","event":"25/09/03 13:56:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.494561","level":"info","event":"25/09/03 13:56:07 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.495344","level":"info","event":"25/09/03 13:56:07 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.496090","level":"info","event":"25/09/03 13:56:07 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.496766","level":"info","event":"25/09/03 13:56:07 INFO SparkContext: Submitted application: ProcessingTask","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.499947","level":"info","event":"25/09/03 13:56:07 INFO SparkContext: Spark configuration:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500128","level":"info","event":"spark.app.name=ProcessingTask","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500206","level":"info","event":"spark.app.startTime=1756907767212","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500263","level":"info","event":"spark.app.submitTime=1756907765961","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500313","level":"info","event":"spark.cores.max=1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500364","level":"info","event":"spark.delta.logStore.class=org.apache.spark.sql.delta.storage.S3SingleDriverLogStore","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500415","level":"info","event":"spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500524","level":"info","event":"spark.eventLog.dir=/opt/spark/history","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500589","level":"info","event":"spark.eventLog.enabled=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500640","level":"info","event":"spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500691","level":"info","event":"spark.executor.memory=512m","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500740","level":"info","event":"spark.hadoop.fs.s3a.access.key=*********(redacted)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500788","level":"info","event":"spark.hadoop.fs.s3a.endpoint=http://minio:9000","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500839","level":"info","event":"spark.hadoop.fs.s3a.fast.upload=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500886","level":"info","event":"spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500935","level":"info","event":"spark.hadoop.fs.s3a.multiobjectdelete.enable=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.500984","level":"info","event":"spark.hadoop.fs.s3a.path.style.access=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501050","level":"info","event":"spark.hadoop.fs.s3a.secret.key=*********(redacted)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501127","level":"info","event":"spark.history.fs.logDirectory=s3a://spark/","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501180","level":"info","event":"spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0,org.postgresql:postgresql:42.7.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501230","level":"info","event":"spark.logConf=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501279","level":"info","event":"spark.master=spark://spark-master:7077","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501327","level":"info","event":"spark.network.timeout=10000s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501375","level":"info","event":"spark.rdd.compress=True","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501423","level":"info","event":"spark.serializer.objectStreamReset=100","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501471","level":"info","event":"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501519","level":"info","event":"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501568","level":"info","event":"spark.sql.files.ignoreMissingFiles=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501615","level":"info","event":"spark.sql.sources.partitionOverwriteMode=dynamic","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501783","level":"info","event":"spark.submit.deployMode=client","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.501898","level":"info","event":"spark.submit.pyFiles=","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.535848","level":"info","event":"25/09/03 13:56:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.545961","level":"info","event":"25/09/03 13:56:07 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.547445","level":"info","event":"25/09/03 13:56:07 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.637576","level":"info","event":"25/09/03 13:56:07 INFO SecurityManager: Changing view acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.638791","level":"info","event":"25/09/03 13:56:07 INFO SecurityManager: Changing modify acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.639639","level":"info","event":"25/09/03 13:56:07 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.640643","level":"info","event":"25/09/03 13:56:07 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:07.641279","level":"info","event":"25/09/03 13:56:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.006122","level":"info","event":"25/09/03 13:56:08 INFO Utils: Successfully started service 'sparkDriver' on port 41149.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.060754","level":"info","event":"25/09/03 13:56:08 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.115543","level":"info","event":"25/09/03 13:56:08 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.137304","level":"info","event":"25/09/03 13:56:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.138060","level":"info","event":"25/09/03 13:56:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.142266","level":"info","event":"25/09/03 13:56:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.169984","level":"info","event":"25/09/03 13:56:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d6d16443-ccfd-45ec-b7bc-6a24ba2a4a32","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.199716","level":"info","event":"25/09/03 13:56:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.218284","level":"info","event":"25/09/03 13:56:08 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.362766","level":"info","event":"25/09/03 13:56:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.445244","level":"info","event":"25/09/03 13:56:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.590032","level":"info","event":"25/09/03 13:56:08 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.659562","level":"info","event":"25/09/03 13:56:08 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 39 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.786731","level":"info","event":"25/09/03 13:56:08 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250903135608-0005","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.802486","level":"info","event":"25/09/03 13:56:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37343.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.802658","level":"info","event":"25/09/03 13:56:08 INFO NettyBlockTransferService: Server created on b2d9647d6a30:37343","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.805978","level":"info","event":"25/09/03 13:56:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.816047","level":"info","event":"25/09/03 13:56:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250903135608-0005/0 on worker-20250903134202-172.18.0.6-43809 (172.18.0.6:43809) with 1 core(s)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.818546","level":"info","event":"25/09/03 13:56:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20250903135608-0005/0 on hostPort 172.18.0.6:43809 with 1 core(s), 512.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.819212","level":"info","event":"25/09/03 13:56:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b2d9647d6a30, 37343, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.823253","level":"info","event":"25/09/03 13:56:08 INFO BlockManagerMasterEndpoint: Registering block manager b2d9647d6a30:37343 with 434.4 MiB RAM, BlockManagerId(driver, b2d9647d6a30, 37343, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.826639","level":"info","event":"25/09/03 13:56:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b2d9647d6a30, 37343, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:08.828480","level":"info","event":"25/09/03 13:56:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b2d9647d6a30, 37343, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:09.151111","level":"info","event":"25/09/03 13:56:09 INFO SingleEventLogFileWriter: Logging events to file:/opt/spark/history/app-20250903135608-0005.inprogress","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:09.363363","level":"info","event":"25/09/03 13:56:09 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:09.731733","level":"info","event":"25/09/03 13:56:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:09.734370","level":"info","event":"25/09/03 13:56:09 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:10.242416","level":"info","event":"25/09/03 13:56:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250903135608-0005/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:11.290747","level":"info","event":"25/09/03 13:56:11 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:11.305836","level":"info","event":"25/09/03 13:56:11 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:11.306045","level":"info","event":"25/09/03 13:56:11 INFO MetricsSystemImpl: s3a-file-system metrics system started","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:12.956814","level":"info","event":"25/09/03 13:56:12 INFO InMemoryFileIndex: It took 94 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.525926","level":"info","event":"25/09/03 13:56:13 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.555233","level":"info","event":"25/09/03 13:56:13 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.556529","level":"info","event":"25/09/03 13:56:13 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.557614","level":"info","event":"25/09/03 13:56:13 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.561574","level":"info","event":"25/09/03 13:56:13 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.571750","level":"info","event":"25/09/03 13:56:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.651591","level":"info","event":"25/09/03 13:56:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 105.5 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.707903","level":"info","event":"25/09/03 13:56:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.712385","level":"info","event":"25/09/03 13:56:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b2d9647d6a30:37343 (size: 38.4 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.719304","level":"info","event":"25/09/03 13:56:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.743031","level":"info","event":"25/09/03 13:56:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:13.744839","level":"info","event":"25/09/03 13:56:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:16.868159","level":"info","event":"25/09/03 13:56:16 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:56326) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:17.151188","level":"info","event":"25/09/03 13:56:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:37063 with 117.0 MiB RAM, BlockManagerId(0, 172.18.0.6, 37063, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:17.214953","level":"info","event":"25/09/03 13:56:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 7757 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:17.642861","level":"info","event":"25/09/03 13:56:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:37063 (size: 38.4 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:21.090100","level":"info","event":"25/09/03 13:56:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3886 ms on 172.18.0.6 (executor 0) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:21.092189","level":"info","event":"25/09/03 13:56:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:21.097945","level":"info","event":"25/09/03 13:56:21 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 7.496 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:21.104150","level":"info","event":"25/09/03 13:56:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:21.104878","level":"info","event":"25/09/03 13:56:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:21.107058","level":"info","event":"25/09/03 13:56:21 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 7.580588 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:21.989715","level":"info","event":"25/09/03 13:56:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on b2d9647d6a30:37343 in memory (size: 38.4 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:22.001944","level":"info","event":"25/09/03 13:56:22 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.6:37063 in memory (size: 38.4 KiB, free: 117.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:23.616507","level":"info","event":"25/09/03 13:56:23 INFO CodeGenerator: Code generated in 319.491464 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.909658","level":"info","event":"25/09/03 13:56:24 INFO CodeGenerator: Code generated in 29.363249 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.929419","level":"info","event":"+--------------------+-------------+-------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.929682","level":"info","event":"|            col_name|    data_type|comment|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.929906","level":"info","event":"+--------------------+-------------+-------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.930101","level":"info","event":"|            VendorID|          int|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.930252","level":"info","event":"|tpep_pickup_datetime|timestamp_ntz|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.930384","level":"info","event":"|tpep_dropoff_date...|timestamp_ntz|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.930523","level":"info","event":"|     passenger_count|       bigint|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.930627","level":"info","event":"|       trip_distance|       double|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.930752","level":"info","event":"|          RatecodeID|       bigint|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.930887","level":"info","event":"|  store_and_fwd_flag|       string|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.931097","level":"info","event":"|        PULocationID|          int|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.931277","level":"info","event":"|        DOLocationID|          int|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.931443","level":"info","event":"|        payment_type|       bigint|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.931663","level":"info","event":"|         fare_amount|       double|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.931854","level":"info","event":"|               extra|       double|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.932041","level":"info","event":"|             mta_tax|       double|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.932208","level":"info","event":"|          tip_amount|       double|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.932380","level":"info","event":"|        tolls_amount|       double|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.932541","level":"info","event":"|improvement_surch...|       double|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.932703","level":"info","event":"|        total_amount|       double|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.932816","level":"info","event":"|congestion_surcharge|       double|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.932906","level":"info","event":"|         Airport_fee|       double|   NULL|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.933021","level":"info","event":"+--------------------+-------------+-------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:24.933151","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:25.787957","level":"info","event":"25/09/03 13:56:25 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:25.789979","level":"info","event":"25/09/03 13:56:25 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.115039","level":"info","event":"25/09/03 13:56:26 INFO CodeGenerator: Code generated in 120.255945 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.170204","level":"info","event":"25/09/03 13:56:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 210.7 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.192702","level":"info","event":"25/09/03 13:56:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.193877","level":"info","event":"25/09/03 13:56:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b2d9647d6a30:37343 (size: 36.3 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.196966","level":"info","event":"25/09/03 13:56:26 INFO SparkContext: Created broadcast 1 from jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.224665","level":"info","event":"25/09/03 13:56:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 27077972 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.373167","level":"info","event":"25/09/03 13:56:26 INFO DAGScheduler: Registering RDD 5 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.383550","level":"info","event":"25/09/03 13:56:26 INFO DAGScheduler: Got map stage job 1 (jdbc at NativeMethodAccessorImpl.java:0) with 2 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.385344","level":"info","event":"25/09/03 13:56:26 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.385997","level":"info","event":"25/09/03 13:56:26 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.386973","level":"info","event":"25/09/03 13:56:26 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.389142","level":"info","event":"25/09/03 13:56:26 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.413462","level":"info","event":"25/09/03 13:56:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 43.6 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.421066","level":"info","event":"25/09/03 13:56:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.423791","level":"info","event":"25/09/03 13:56:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b2d9647d6a30:37343 (size: 19.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.424775","level":"info","event":"25/09/03 13:56:26 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.428563","level":"info","event":"25/09/03 13:56:26 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.428806","level":"info","event":"25/09/03 13:56:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.440279","level":"info","event":"25/09/03 13:56:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 8214 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:26.563070","level":"info","event":"25/09/03 13:56:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.6:37063 (size: 19.1 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:29.970234","level":"info","event":"25/09/03 13:56:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:37063 (size: 36.3 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:32.462944","level":"info","event":"25/09/03 13:56:32 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.6, executor 0, partition 1, PROCESS_LOCAL, 8214 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:32.473326","level":"info","event":"25/09/03 13:56:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 6042 ms on 172.18.0.6 (executor 0) (1/2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.170708","level":"info","event":"25/09/03 13:56:33 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 708 ms on 172.18.0.6 (executor 0) (2/2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.171079","level":"info","event":"25/09/03 13:56:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.173097","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: ShuffleMapStage 1 (jdbc at NativeMethodAccessorImpl.java:0) finished in 6.776 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.174136","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.174956","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.175877","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.176700","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.239808","level":"info","event":"25/09/03 13:56:33 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.261237","level":"info","event":"25/09/03 13:56:33 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.316226","level":"info","event":"25/09/03 13:56:33 INFO CodeGenerator: Code generated in 33.016721 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.382817","level":"info","event":"25/09/03 13:56:33 INFO CodeGenerator: Code generated in 18.02453 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.429405","level":"info","event":"25/09/03 13:56:33 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.431838","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Got job 2 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.432036","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Final stage: ResultStage 3 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.432132","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.432773","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.433828","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.450198","level":"info","event":"25/09/03 13:56:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 46.5 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.459042","level":"info","event":"25/09/03 13:56:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 20.8 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.459473","level":"info","event":"25/09/03 13:56:33 INFO BlockManagerInfo: Removed broadcast_2_piece0 on b2d9647d6a30:37343 in memory (size: 19.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.460656","level":"info","event":"25/09/03 13:56:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b2d9647d6a30:37343 (size: 20.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.462470","level":"info","event":"25/09/03 13:56:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.463965","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.464226","level":"info","event":"25/09/03 13:56:33 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.464410","level":"info","event":"25/09/03 13:56:33 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.6:37063 in memory (size: 19.1 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.474005","level":"info","event":"25/09/03 13:56:33 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.18.0.6, executor 0, partition 0, NODE_LOCAL, 7619 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.500450","level":"info","event":"25/09/03 13:56:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.6:37063 (size: 20.8 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.671409","level":"info","event":"25/09/03 13:56:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.6:56326","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.970549","level":"info","event":"25/09/03 13:56:33 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 499 ms on 172.18.0.6 (executor 0) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.970865","level":"info","event":"25/09/03 13:56:33 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.972524","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: ResultStage 3 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.525 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.973171","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.973399","level":"info","event":"25/09/03 13:56:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.974313","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Job 2 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.544213 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.984718","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Registering RDD 11 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.985067","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Got map stage job 3 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.985197","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.985280","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.985636","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:33.986482","level":"info","event":"25/09/03 13:56:33 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[11] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.001445","level":"info","event":"25/09/03 13:56:34 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 47.2 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.009158","level":"info","event":"25/09/03 13:56:34 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.2 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.010915","level":"info","event":"25/09/03 13:56:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on b2d9647d6a30:37343 (size: 21.2 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.011887","level":"info","event":"25/09/03 13:56:34 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.014708","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[11] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.014972","level":"info","event":"25/09/03 13:56:34 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.015146","level":"info","event":"25/09/03 13:56:34 INFO BlockManagerInfo: Removed broadcast_3_piece0 on b2d9647d6a30:37343 in memory (size: 20.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.016213","level":"info","event":"25/09/03 13:56:34 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.6:37063 in memory (size: 20.8 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.016476","level":"info","event":"25/09/03 13:56:34 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (172.18.0.6, executor 0, partition 0, NODE_LOCAL, 7608 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.054974","level":"info","event":"25/09/03 13:56:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.6:37063 (size: 21.2 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.263165","level":"info","event":"25/09/03 13:56:34 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 248 ms on 172.18.0.6 (executor 0) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.263449","level":"info","event":"25/09/03 13:56:34 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.265368","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: ShuffleMapStage 5 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.272 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.265666","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.265932","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.266137","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.266296","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.272314","level":"info","event":"25/09/03 13:56:34 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.296244","level":"info","event":"25/09/03 13:56:34 INFO CodeGenerator: Code generated in 12.732262 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.370543","level":"info","event":"25/09/03 13:56:34 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.373228","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: Got job 4 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.373449","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: Final stage: ResultStage 8 (jdbc at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.373544","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.374137","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.375184","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[16] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.387432","level":"info","event":"25/09/03 13:56:34 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 50.5 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.394992","level":"info","event":"25/09/03 13:56:34 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 434.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.397851","level":"info","event":"25/09/03 13:56:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on b2d9647d6a30:37343 (size: 23.5 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.399847","level":"info","event":"25/09/03 13:56:34 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.400102","level":"info","event":"25/09/03 13:56:34 INFO BlockManagerInfo: Removed broadcast_4_piece0 on b2d9647d6a30:37343 in memory (size: 21.2 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.403084","level":"info","event":"25/09/03 13:56:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[16] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.403479","level":"info","event":"25/09/03 13:56:34 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.405169","level":"info","event":"25/09/03 13:56:34 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.18.0.6:37063 in memory (size: 21.2 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.405390","level":"info","event":"25/09/03 13:56:34 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (172.18.0.6, executor 0, partition 0, NODE_LOCAL, 7619 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.448109","level":"info","event":"25/09/03 13:56:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.6:37063 (size: 23.5 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:34.856016","level":"info","event":"25/09/03 13:56:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.6:56326","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.567188","level":"info","event":"25/09/03 13:56:35 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 1162 ms on 172.18.0.6 (executor 0) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.567459","level":"info","event":"25/09/03 13:56:35 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.569229","level":"info","event":"25/09/03 13:56:35 INFO DAGScheduler: ResultStage 8 (jdbc at NativeMethodAccessorImpl.java:0) finished in 1.191 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.569672","level":"info","event":"25/09/03 13:56:35 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.569895","level":"info","event":"25/09/03 13:56:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.570603","level":"info","event":"25/09/03 13:56:35 INFO DAGScheduler: Job 4 finished: jdbc at NativeMethodAccessorImpl.java:0, took 1.199566 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.680498","level":"info","event":"25/09/03 13:56:35 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.680688","level":"info","event":"25/09/03 13:56:35 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.739278","level":"info","event":"25/09/03 13:56:35 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 210.7 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.754301","level":"info","event":"25/09/03 13:56:35 INFO BlockManagerInfo: Removed broadcast_5_piece0 on b2d9647d6a30:37343 in memory (size: 23.5 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.759294","level":"info","event":"25/09/03 13:56:35 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.6:37063 in memory (size: 23.5 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.759564","level":"info","event":"25/09/03 13:56:35 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.761328","level":"info","event":"25/09/03 13:56:35 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on b2d9647d6a30:37343 (size: 36.3 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.763481","level":"info","event":"25/09/03 13:56:35 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.766692","level":"info","event":"25/09/03 13:56:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 27077972 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.777121","level":"info","event":"25/09/03 13:56:35 INFO DAGScheduler: Registering RDD 20 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.777351","level":"info","event":"25/09/03 13:56:35 INFO DAGScheduler: Got map stage job 5 (showString at NativeMethodAccessorImpl.java:0) with 2 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.777561","level":"info","event":"25/09/03 13:56:35 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.777718","level":"info","event":"25/09/03 13:56:35 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.777926","level":"info","event":"25/09/03 13:56:35 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.778148","level":"info","event":"25/09/03 13:56:35 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.784137","level":"info","event":"25/09/03 13:56:35 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 45.0 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.788148","level":"info","event":"25/09/03 13:56:35 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 433.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.790196","level":"info","event":"25/09/03 13:56:35 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on b2d9647d6a30:37343 (size: 19.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.791379","level":"info","event":"25/09/03 13:56:35 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.792434","level":"info","event":"25/09/03 13:56:35 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.792710","level":"info","event":"25/09/03 13:56:35 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.795112","level":"info","event":"25/09/03 13:56:35 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 8214 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.822708","level":"info","event":"25/09/03 13:56:35 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.6:37063 (size: 19.8 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:35.873271","level":"info","event":"25/09/03 13:56:35 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.6:37063 (size: 36.3 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.350117","level":"info","event":"25/09/03 13:56:36 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 7) (172.18.0.6, executor 0, partition 1, PROCESS_LOCAL, 8214 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.351467","level":"info","event":"25/09/03 13:56:36 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 556 ms on 172.18.0.6 (executor 0) (1/2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.655515","level":"info","event":"25/09/03 13:56:36 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 7) in 306 ms on 172.18.0.6 (executor 0) (2/2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.655775","level":"info","event":"25/09/03 13:56:36 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.656996","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: ShuffleMapStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 0.877 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.657260","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.657549","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.657742","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.657908","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.667865","level":"info","event":"25/09/03 13:56:36 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.680529","level":"info","event":"25/09/03 13:56:36 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.716124","level":"info","event":"25/09/03 13:56:36 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.718128","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.718393","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: Final stage: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.718607","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.718723","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.719704","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[24] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.725706","level":"info","event":"25/09/03 13:56:36 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 46.5 KiB, free 433.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.733472","level":"info","event":"25/09/03 13:56:36 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 21.0 KiB, free 433.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.734910","level":"info","event":"25/09/03 13:56:36 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on b2d9647d6a30:37343 (size: 21.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.736773","level":"info","event":"25/09/03 13:56:36 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.737060","level":"info","event":"25/09/03 13:56:36 INFO BlockManagerInfo: Removed broadcast_7_piece0 on b2d9647d6a30:37343 in memory (size: 19.8 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.737261","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[24] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.737399","level":"info","event":"25/09/03 13:56:36 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.739716","level":"info","event":"25/09/03 13:56:36 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (172.18.0.6, executor 0, partition 0, NODE_LOCAL, 7619 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.740067","level":"info","event":"25/09/03 13:56:36 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.6:37063 in memory (size: 19.8 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.761316","level":"info","event":"25/09/03 13:56:36 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.6:37063 (size: 21.0 KiB, free: 116.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.788319","level":"info","event":"25/09/03 13:56:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.6:56326","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.866963","level":"info","event":"25/09/03 13:56:36 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 128 ms on 172.18.0.6 (executor 0) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.867293","level":"info","event":"25/09/03 13:56:36 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.869282","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 0.146 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.869791","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.869989","level":"info","event":"25/09/03 13:56:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.870675","level":"info","event":"25/09/03 13:56:36 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.154191 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.887438","level":"info","event":"25/09/03 13:56:36 INFO CodeGenerator: Code generated in 11.309127 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.893830","level":"info","event":"+---------------+------------------+---------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.894091","level":"info","event":"|passenger_count|      avg_distance|num_trips|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.894241","level":"info","event":"+---------------+------------------+---------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.894371","level":"info","event":"|           NULL|11.674403475977822|   140162|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.894495","level":"info","event":"|              0|2.7438750993166874|    31465|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.894653","level":"info","event":"|              1|3.1375658449909207|  2188739|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.894781","level":"info","event":"|              2| 3.782764037787955|   405103|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.894894","level":"info","event":"|              3| 3.664591615349205|    91262|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.895010","level":"info","event":"|              4|  3.87591141724711|    51974|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.895139","level":"info","event":"|              5|3.0734722139318427|    33506|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.895273","level":"info","event":"|              6|2.9516888113452344|    22353|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.895402","level":"info","event":"|              7|           2.29375|        8|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.895556","level":"info","event":"|              8|1.5539215686274512|       51|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.895720","level":"info","event":"|              9|               1.8|        1|","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.895865","level":"info","event":"+---------------+------------------+---------+","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.895996","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.944697","level":"info","event":"25/09/03 13:56:36 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.945434","level":"info","event":"25/09/03 13:56:36 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.960858","level":"info","event":"25/09/03 13:56:36 INFO SparkUI: Stopped Spark web UI at http://b2d9647d6a30:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.965507","level":"info","event":"25/09/03 13:56:36 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.967118","level":"info","event":"25/09/03 13:56:36 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:36.993692","level":"info","event":"25/09/03 13:56:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.022829","level":"info","event":"25/09/03 13:56:37 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.023153","level":"info","event":"25/09/03 13:56:37 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.026419","level":"info","event":"25/09/03 13:56:37 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.030488","level":"info","event":"25/09/03 13:56:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.047193","level":"info","event":"25/09/03 13:56:37 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.047532","level":"info","event":"25/09/03 13:56:37 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.048029","level":"info","event":"25/09/03 13:56:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f050bd4-3129-4b1d-a2c6-51ac547ecd2b","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.057922","level":"info","event":"25/09/03 13:56:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5412fb0-154e-4149-b964-ed1e486b066b/pyspark-859494b4-22aa-4bde-ba6d-af16a0909417","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.066541","level":"info","event":"25/09/03 13:56:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5412fb0-154e-4149-b964-ed1e486b066b","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.076999","level":"info","event":"25/09/03 13:56:37 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.077324","level":"info","event":"25/09/03 13:56:37 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-09-03T13:56:37.077459","level":"info","event":"25/09/03 13:56:37 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
